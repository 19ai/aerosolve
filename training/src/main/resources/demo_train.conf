job_name : "Generic Pipeline"

training_data_version : 1
model_version: "a"

// Which model config to use
model_type : "linear"
model_config : ${model_type}"_model_config"

// Hive table where the data live
demo_table : "income_training_raw"

// Update this based on your HDFS directory layout
prefix : "hdfs://"

training_data : ${prefix}"/training_data"${training_data_version}
eval_output : ${prefix}"/eval_output/"${training_data_version}_${model_version}".eval"
model_name : ${prefix}"/model/"${training_data_version}_${model_version}_${model_type}".model"
model_dump : ${model_name}".tsv"
calibrated_model_name : ${prefix}"/calibrated_model/"${training_data_version}_${model_version}".model"

train_subsample : 0.1
eval_subsample : 0.1
scoring_output : ${prefix}"/scores/"${training_data_version}

// The convention is to name features FAMILY_SHORTNAME
// The reason for feature familities is so we can apply transforms to families at a time.
// By default two families are created
// BIAS, B and
// MISS, COLUMN_NAME for missing features
// The exception is a special feature named "LABEL" which also has to be a double
// For the example query, we use two family, i_ as number feature, and s_ as string feature

generic_hive_query : """
  select
    if(LENGTH(label) = 5, -1, 1) as LABEL,
    age as i_age,
    workclass as s_workclass, fnlwgt as s_fnlwgt,
    education as s_education,
    education_num as i_educationnum,
    marital_status as s_marital,
    occupation as s_occupation,
    relationship as s_relationship,
    race as s_race, sex as s_sex,
    capital_gain,
    capital_loss,
    hours_per_week as i_hours,
    native_country as s_country
"""

debug_example {
  hive_query : ${generic_hive_query}" from "${demo_table}
  count : 10
}

debug_transforms {
  hive_query : ${generic_hive_query}" from "${demo_table}
  // Which model config to use when debugging
  model_config : ${model_config}
  count : 3
}

make_training {
  hive_query : ${generic_hive_query}" from "${demo_table}
  num_shards : 20
  output : ${training_data}
}

train_model {
  input : ${training_data}
  subsample : ${train_subsample}
  model_config : ${model_config}
}

eval_model {
  input : ${training_data}
  subsample : ${eval_subsample}
  bins : 5
  model_config : ${model_config}
  is_probability : false
  is_regression : false
  metric_to_maximize : "!HOLD_F1"
  model_name : ${model_name}
}

calibrate_model {
  model_config : ${model_config}
  model_name : ${model_name}
  calibrated_model_output : ${calibrated_model_name}
  input: ${training_data}
  learning_rate : 0.01
  rate_decay : 0.95
  iterations : 100
  num_bags : 10
  tolerance : 0.01
  subsample : 0.5
}

eval_model_calibrated {
  input : ${training_data}
  subsample : ${eval_subsample}
  bins : 5
  model_config : ${model_config}
  is_probability : true
  metric_to_maximize : "!HOLD_F1"
  model_name : ${calibrated_model_name}
}

dump_model {
  // Weights are the same calibrated or not it's just the probabilities that
  // are different
  model_name : ${model_name}
  model_dump : ${model_dump}
}

score_table {
  model_config : ${model_config}
  model_name : ${calibrated_model_name}
  hive_query : ${generic_hive_query}", id as UNIQUE_ID from training.demo_set"
  num_shards : 20
  output : ${scoring_output}
}

debug_score_table {
  model_config : ${model_config}
  model_name : ${calibrated_model_name}
  hive_query : ${generic_hive_query}", id as UNIQUE_ID from training.demo_set"
  count : 10
}

identity_transform {
  transform : list
  transforms : [ ]
}

quantize_capital {
  transform : multiscale_quantize
  field1 : "capital"
  output : "capital"
  buckets : [500, 2000]
}

move_age {
  transform : multiscale_move_float_to_string
  field1 : "i"
  keys: [
    "age"
  ]
  output : "A"
  buckets : [5.0]
}

move_hours {
  transform : multiscale_move_float_to_string
  field1 : "i"
  keys: [
    "hours"
  ]
  output : "A"
  buckets : [5.0]
}

move_edu {
  transform : multiscale_move_float_to_string
  field1 : "i"
  keys: [
    "educationnum"
  ]
  output : "A"
  buckets : [3.0]
}


// Does this listing like people coming from a certain country?
id_x_user {
  transform : cross
  field1 : "ID"
  field2 : "USER"
  output : "ID_x_USER"
}

combined_transform {
  transform : list
  transforms : [
    "quantize_capital"
    "move_age"
    "move_hours"
    "move_edu"
  ]
}

// Config for a linear product quantization model. This model is heavily
// dependent upon feature transforms being set up correctly as it only
// handles discrete (i.e.) string features and all float features
// have to be quantized. This is so that we can debug the model easily.
linear_model_config {
  trainer : "linear"
  prior : [
    "BIAS,B,0.0", // Set the priors using feature family, feature name, initial weight
  ]
  model_output : ${model_name}
  rank_key : "LABEL"
  loss : "hinge"
  margin : 1.0
  rank_threshold : 0.5
  dropout : 0.1
  learning_rate : 0.01
  num_bags : 10
  iterations : 10
  lambda : 0.001
  lambda2 : 0.01
  context_transform : identity_transform
  item_transform : identity_transform
  combined_transform : combined_transform
}

// Forest of trees model. Less feature engineering needed but less debuggable.
// Use this if you just want something fast and easy to use.
forest_model_config {
  trainer : "forest"
  model_output : ${model_name}
  rank_key : "LABEL"
  rank_threshold : 0.5
  // How many trees
  num_trees : 100
  // How many samples to use per tree
  num_candidates : 100000
  // Max depth of a tree
  max_depth : 6
  // Minimum number of samples in each leaf
  min_leaf_items : 3000
  // How many times per split to search for an optimal split
  num_tries : 100
  context_transform : identity_transform
  item_transform : identity_transform
  combined_transform : identity_transform
}

param_search {
  // model type
  model_config : ${model_config}
  // optimization target
  metric_to_maximize : "!HOLD_F1"
  // path and prefix used to store all models
  model_name_prefix : ${model_name}
  // max number of trials in parameter search, each round has one training and one eval
  max_round: 8
  // strategies to conduct parameter search. "guided" will take all provide values and
  // cross join them. "grid" will take an optional min (first val) and an optional max
  // (second val) and generate equal splits for each parameter based on max_round.
  search_strategy: "guided"
  // list of parameters to tune (defined in model_config). In "guided" stategy, provide
  // all parameters you want to try. In "grid" strategy, provide two optional values:
  // first is a min (default 0), second is max (default 1).
  param_to_tune: [
    {
      name: "lambda"
      val: [0.001, 0.01, 0.1]
    }
    {
      name: "lambda2"
      val: [0.01, 0.06, 0.1]
    }
  ]
  // example for grid strategy
  //  param_to_tune: [
  //    {
  //      name: "lambda"
  //      val: [0.001, 0.1]
  //    }
  //    {
  //      name: "lambda2"
  //      val: [0.01, 0.1]
  //    }
  //  ]
  // optional, where you want to store the results. If omitted, results will not be stored.
  // Results will be displayed in terminal log regardlessly.
  output: ${prefix}"/paramsearch/"${training_data_version}_${model_version}_${model_type}".txt"
  // optional, where you want to copy the best model to.
  best_model_output: ${model_name}
}